{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bfaa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced VGG16 Visualization for Understanding Convolutional Neural Networks\n",
    "This notebook demonstrates how CNNs learn hierarchical features from images.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85552b-99d3-42f6-b0d7-2368142a3876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set matplotlib style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ae555-cf7a-4f9a-92b1-be5ef2f94121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: Load and Prepare the VGG16 Model\n",
    "# ============================================================================\n",
    "\n",
    "def load_vgg16_model():\n",
    "    \"\"\"\n",
    "    Load the pre-trained VGG16 model.\n",
    "    VGG16 is a 16-layer deep CNN trained on ImageNet dataset.\n",
    "    It learns to recognize 1000 different object categories.\n",
    "    \"\"\"\n",
    "    print(\"Loading VGG16 model (this may take a moment)...\")\n",
    "    model = VGG16(weights='imagenet')\n",
    "    print(f\"Model loaded successfully! Total layers: {len(model.layers)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8b877-06ab-40a7-80fa-4e979f488905",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = load_vgg16_model()\n",
    "\n",
    "# Display model architecture summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VGG16 ARCHITECTURE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5baa72-3941-4e10-a3c2-e3eeb82a7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: Understanding the Convolutional Layers\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_conv_layers(model):\n",
    "    \"\"\"\n",
    "    Analyze and display information about all convolutional layers.\n",
    "    This helps us understand the architecture of the network.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CONVOLUTIONAL LAYERS ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    conv_layers = []\n",
    "    for idx, layer in enumerate(model.layers):\n",
    "        if 'conv' in layer.name:\n",
    "            filters, biases = layer.get_weights()\n",
    "            conv_layers.append({\n",
    "                'index': idx,\n",
    "                'name': layer.name,\n",
    "                'filters_shape': filters.shape,\n",
    "                'num_filters': filters.shape[3]\n",
    "            })\n",
    "            print(f\"Layer {idx}: {layer.name}\")\n",
    "            print(f\"  Filter shape: {filters.shape}\")\n",
    "            print(f\"  Number of filters: {filters.shape[3]}\")\n",
    "            print(f\"  Interpretation: This layer has {filters.shape[3]} different \")\n",
    "            print(f\"                  feature detectors, each looking for specific patterns\\n\")\n",
    "    \n",
    "    return conv_layers\n",
    "\n",
    "conv_layer_info = analyze_conv_layers(full_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a58625-8dec-409a-be21-d2ce44d075f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: Load and Preprocess Input Image\n",
    "# ============================================================================\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Load an image and prepare it for VGG16 processing.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the input image\n",
    "        target_size: Size to resize image (VGG16 requires 224x224)\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed image ready for model input\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Image file '{image_path}' not found. \"\n",
    "            f\"Please provide a valid image path.\"\n",
    "        )\n",
    "    \n",
    "    # Load and resize image\n",
    "    img = load_img(image_path, target_size=target_size)\n",
    "    \n",
    "    # Convert to array and add batch dimension\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Preprocess for VGG16 (subtract ImageNet mean)\n",
    "    img_preprocessed = preprocess_input(img_array)\n",
    "    \n",
    "    return img, img_preprocessed\n",
    "\n",
    "# Try to load the image with error handling\n",
    "try:\n",
    "    original_img, processed_img = load_and_preprocess_image('me.jpg')\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"IMAGE LOADED SUCCESSFULLY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Original image size: {original_img.size}\")\n",
    "    print(f\"Preprocessed array shape: {processed_img.shape}\")\n",
    "    \n",
    "    # Display the original image\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(original_img)\n",
    "    plt.title(\"Original Input Image\", fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('01_original_image.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(\"Please ensure you have an image named 'me.jpg' in the working directory.\")\n",
    "    print(\"You can change the filename in the code to match your image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df1aca-3e65-44ca-9f3b-63e05b50b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: Visualize First Layer Filters (What the Network Looks For)\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_first_layer_filters(model, num_filters=8):\n",
    "    \"\"\"\n",
    "    Visualize the learned filters in the first convolutional layer.\n",
    "    These filters detect basic features like edges, colors, and textures.\n",
    "    \n",
    "    The first layer learns simple patterns because it sees raw pixel values.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FIRST LAYER FILTER VISUALIZATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get filters from the first convolutional layer\n",
    "    filters, biases = model.layers[1].get_weights()\n",
    "    \n",
    "    # Normalize filter values to [0, 1] for visualization\n",
    "    f_min, f_max = filters.min(), filters.max()\n",
    "    filters_normalized = (filters - f_min) / (f_max - f_min)\n",
    "    \n",
    "    print(f\"Visualizing {num_filters} filters from the first conv layer\")\n",
    "    print(f\"Each filter has 3 channels (R, G, B)\")\n",
    "    print(f\"These filters detect basic features like edges and color gradients\\n\")\n",
    "   \n",
    "    # Create visualization\n",
    "    fig = plt.figure(figsize=(15, num_filters * 1.5))\n",
    "    \n",
    "    for i in range(num_filters):\n",
    "        f = filters_normalized[:, :, :, i]\n",
    "        \n",
    "        # Display each color channel separately\n",
    "        for j in range(3):\n",
    "            ax = plt.subplot(num_filters, 3, i * 3 + j + 1)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "            # Add labels for the first row\n",
    "            if i == 0:\n",
    "                channel_names = ['Red Channel', 'Green Channel', 'Blue Channel']\n",
    "                ax.set_title(channel_names[j], fontsize=10, fontweight='bold')\n",
    "            \n",
    "            # Add filter number label\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(f'Filter {i+1}', fontsize=10, fontweight='bold')\n",
    "            \n",
    "            plt.imshow(f[:, :, j], cmap='gray')\n",
    "    \n",
    "    plt.suptitle('First Layer Convolutional Filters (Learned Feature Detectors)', \n",
    "                 fontsize=14, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('02_first_layer_filters.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return filters_normalized\n",
    "try:\n",
    "    first_layer_filters = visualize_first_layer_filters(full_model, num_filters=8)\n",
    "except Exception as e:\n",
    "    print(f\"Error visualizing filters: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aa4355-21a2-4b01-9b1a-7fa141cbcec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: Extract and Visualize Feature Maps from Multiple Layers\n",
    "# ============================================================================\n",
    "\n",
    "def create_feature_extractor(model, layer_name):\n",
    "    \"\"\"\n",
    "    Create a model that outputs feature maps from a specific layer.\n",
    "    This allows us to see what the network \"sees\" at different depths.\n",
    "    \"\"\"\n",
    "    layer_output = model.get_layer(layer_name).output\n",
    "    return Model(inputs=model.input, outputs=layer_output)\n",
    "\n",
    "def visualize_feature_maps(feature_maps, layer_name, num_features=64, grid_size=8):\n",
    "    \"\"\"\n",
    "    Visualize the feature maps (activations) from a convolutional layer.\n",
    "    \n",
    "    Feature maps show which parts of the image activate each filter.\n",
    "    - Bright areas: Strong activation (filter detected its pattern)\n",
    "    - Dark areas: Weak activation (pattern not present)\n",
    "    \"\"\"\n",
    "    print(f\"\\nVisualizing {num_features} feature maps from {layer_name}\")\n",
    "    print(f\"Feature maps shape: {feature_maps.shape}\")\n",
    "    \n",
    "    # Create figure with proper size\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        ax = plt.subplot(grid_size, grid_size, i + 1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # Display the feature map\n",
    "        plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\n",
    "    \n",
    "    plt.suptitle(f'Feature Maps from {layer_name}\\n' + \n",
    "                 'Bright areas show where the filter detected its pattern',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save with a filename based on layer name\n",
    "    filename = f'03_feature_maps_{layer_name}.png'\n",
    "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize feature maps from multiple layers to show hierarchical learning\n",
    "try:\n",
    "    layers_to_visualize = [\n",
    "        ('block1_conv1', 64, 8),  # Early layer: edges, colors\n",
    "        ('block2_conv1', 64, 8),  # Middle layer: textures, simple shapes\n",
    "        ('block3_conv1', 64, 8),  # Deeper layer: complex patterns\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FEATURE MAPS VISUALIZATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Feature maps show what each filter 'sees' in the image.\")\n",
    "    print(\"Early layers detect simple features, deeper layers detect complex patterns.\\n\")\n",
    "    \n",
    "    for layer_name, num_features, grid_size in layers_to_visualize:\n",
    "        # Create feature extractor for this layer\n",
    "        feature_extractor = create_feature_extractor(full_model, layer_name)\n",
    "        \n",
    "        # Extract features\n",
    "        features = feature_extractor.predict(processed_img, verbose=0)\n",
    "        \n",
    "        # Visualize\n",
    "        visualize_feature_maps(features, layer_name, num_features, grid_size)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during feature extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4391d0eb-ed09-4078-991d-151b676b7e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: Compare Multiple Layers Side by Side\n",
    "# ============================================================================\n",
    "\n",
    "def compare_layer_depths(model, image, layers=['block1_conv1', 'block3_conv1', 'block5_conv1']):\n",
    "    \"\"\"\n",
    "    Compare how different depth layers respond to the same image.\n",
    "    This demonstrates the hierarchical nature of CNN feature learning.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"HIERARCHICAL FEATURE LEARNING COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(layers) + 1, figsize=(20, 4))\n",
    "    \n",
    "    # Show original image\n",
    "    axes[0].imshow(original_img)\n",
    "    axes[0].set_title('Original Image', fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Show feature maps from each layer\n",
    "    for idx, layer_name in enumerate(layers):\n",
    "        extractor = create_feature_extractor(model, layer_name)\n",
    "        features = extractor.predict(image, verbose=0)\n",
    "        \n",
    "        # Show the first feature map from each layer\n",
    "        axes[idx + 1].imshow(features[0, :, :, 0], cmap='viridis')\n",
    "        axes[idx + 1].set_title(f'{layer_name}\\n(Depth: Layer {idx + 1})', \n",
    "                                fontsize=12, fontweight='bold')\n",
    "        axes[idx + 1].axis('off')\n",
    "    \n",
    "    plt.suptitle('How CNN Sees the Image at Different Depths\\n' +\n",
    "                 'Notice how features become more abstract in deeper layers',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('04_hierarchical_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- Block 1: Detects edges, colors, and basic textures\")\n",
    "    print(\"- Block 3: Detects more complex shapes and patterns\")\n",
    "    print(\"- Block 5: Detects high-level features and object parts\")\n",
    "\n",
    "try:\n",
    "    compare_layer_depths(full_model, processed_img)\n",
    "except Exception as e:\n",
    "    print(f\"Error in comparison: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770090bc-3143-4621-8003-a314cc6f54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: Summary and Key Takeaways\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY TAKEAWAYS ABOUT CNNs\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. HIERARCHICAL LEARNING: CNNs learn features in a hierarchy\n",
    "   - Early layers: Simple features (edges, colors)\n",
    "   - Middle layers: Textures and patterns\n",
    "   - Deep layers: Complex objects and concepts\n",
    "\n",
    "2. FILTERS ARE FEATURE DETECTORS: Each filter learns to detect a specific pattern\n",
    "   - The network automatically learns these patterns from data\n",
    "   - No manual feature engineering needed\n",
    "\n",
    "3. FEATURE MAPS SHOW ACTIVATION: Bright areas in feature maps indicate\n",
    "   where the filter found its pattern in the image\n",
    "\n",
    "4. TRANSLATION INVARIANCE: CNNs can detect features regardless of their\n",
    "   position in the image due to the convolutional structure\n",
    "\n",
    "5. PARAMETER SHARING: The same filter is applied across the entire image,\n",
    "   making CNNs efficient and good at generalization\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
